{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f73124e-ac9a-4e48-abf9-d07b7e719443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28fce629-7310-4bb4-bde1-8bc6aa521d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('archive/Sentiment.csv')\n",
    "# Keeping only the necessary columns\n",
    "data = data[['text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1ca21-e4dd-4ec7-a1ad-ccc8ba2c2be9",
   "metadata": {},
   "source": [
    "We will create a function to remove unwanted charactrs in Tweets using Regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bbbd3de-930a-40f6-970c-eb8b024e856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess_data(text):\n",
    "    text = text.lower()\n",
    "    new_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    new_text = re.sub('rt', '', new_text)\n",
    "    return new_text\n",
    "\n",
    "data['text'] = data['text'].apply(preProcess_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b7b40-f6b7-479e-af2b-86c1273471cc",
   "metadata": {},
   "source": [
    "We will use TensorFlow's tokenizer to tokenize our dataset, and TensorFlow's pad_sequences to pad our sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52f6137a-7df1-4fe4-91a2-6af2bbebd15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 2000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "# print(X[:5])\n",
    "X = pad_sequences(X, 28) \n",
    "# print(X[:5])\n",
    "\n",
    "Y = pd.get_dummies(data['sentiment']).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ced58-96f8-4550-96c0-190482410ee9",
   "metadata": {},
   "source": [
    "Now we will split the dataset into training and testing portions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87fce16d-5834-452f-bcca-613b337aee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "141fc0c8-a751-4ed1-b562-947d2d80d513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d107c-dd23-4495-9553-65ec1dedabc9",
   "metadata": {},
   "source": [
    "It is now time to design and create the deep learning model. We will simply use an embedding layer and some LSTM layers with dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64e612dd-f217-4855-ac22-d64a9c8bcf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(128,recurrent_dropout=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91768655-5e20-49ca-8245-40b92bbf3b0c",
   "metadata": {},
   "source": [
    "We will now fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f3b7aa3-b6c4-419d-86ee-bc2110edd32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 9s 304ms/step - loss: 0.9643 - accuracy: 0.5978 - val_loss: 0.9017 - val_accuracy: 0.6054\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 6s 291ms/step - loss: 0.8509 - accuracy: 0.6236 - val_loss: 0.8078 - val_accuracy: 0.6396\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 6s 295ms/step - loss: 0.7493 - accuracy: 0.6747 - val_loss: 0.7696 - val_accuracy: 0.6674\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 6s 291ms/step - loss: 0.6908 - accuracy: 0.7031 - val_loss: 0.7419 - val_accuracy: 0.6768\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 6s 291ms/step - loss: 0.6398 - accuracy: 0.7254 - val_loss: 0.7405 - val_accuracy: 0.6732\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 7s 299ms/step - loss: 0.6104 - accuracy: 0.7394 - val_loss: 0.7419 - val_accuracy: 0.6732\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 7s 297ms/step - loss: 0.5945 - accuracy: 0.7470 - val_loss: 0.7450 - val_accuracy: 0.6775\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 7s 296ms/step - loss: 0.5801 - accuracy: 0.7505 - val_loss: 0.7641 - val_accuracy: 0.6728\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 6s 292ms/step - loss: 0.5684 - accuracy: 0.7602 - val_loss: 0.7750 - val_accuracy: 0.6757\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 7s 303ms/step - loss: 0.5483 - accuracy: 0.7678 - val_loss: 0.7773 - val_accuracy: 0.6677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa5d4a7e460>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 512\n",
    "\n",
    "model.fit(X_train, Y_train, \n",
    "          epochs = 10, \n",
    "          batch_size=batch_size, \n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f724e4-e8fb-4087-8eea-b33f394b84d0",
   "metadata": {},
   "source": [
    "Now the deep learning model is trained, we will save the model so that we do not have to train every time we reload our server. Instead, we just use the trained model. Note that I have not done much hyper-parameter tuning or model improvement, as you can do it by yourself to deploy an improved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ccb96ce0-ad6d-4ff1-8da8-7f1b4f865224",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentiment.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfbd627-f114-4247-bb4d-284209bc7ea2",
   "metadata": {},
   "source": [
    "### Step 3: Creating a REST API using FAST API\n",
    "\n",
    "We will create a REST API using FAST API. We will create a new file named app.py. We will first do the important imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ba603a98-2455-485a-b1f4-228e700bb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastapi import FastAPI, Form\n",
    "import pandas as pd\n",
    "from starlette.responses import HTMLResponse\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1229ca0-4afe-4277-9b65-1cb7949a8bf8",
   "metadata": {},
   "source": [
    "Here we have imported FastAPI and Form from the fast API library, using which we will create an Input Form and endpoint for our API. We have imported HTMLResponse from starlette.response, which will help in creating an input form.\n",
    "\n",
    "We will start by creating an input form so that users can input data, i.e., a test string on which we can test the sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53060bb5-83c4-4e43-a40b-bfcdee8fbc81",
   "metadata": {},
   "source": [
    "We have created our FastAPI app in the first line and used the get method on the /predict route, which will return an HTML response so that the user can see a real HTML page, and input the data on forms using the post method. We will use that data to predict on.\n",
    "\n",
    "You can run your app now by running the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c444799b-dc18-4a58-8d1b-3319a13cfc78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
